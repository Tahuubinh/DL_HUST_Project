{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cb96447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "# tf.random.set_seed(123)\n",
    "# np.random.seed(123)\n",
    "from sklearn import metrics\n",
    "import json \n",
    "import os\n",
    "\n",
    "def timeseries_evaluation_metrics_func(y_true, y_pred):\n",
    "    \n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print('Evaluation metric results:-')\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "    print(f'MSE is : {mse}')\n",
    "    print(f'MAE is : {mae}')\n",
    "    print(f'RMSE is : {rmse}')\n",
    "    print(f'MAPE is : {mape}')\n",
    "    print(f'R2 is : {r2}',end='\\n\\n')\n",
    "    return {'mse' : mse, 'mae' : mae, 'rmse' : rmse, 'mape' : mape, 'r2' : r2}\n",
    "    \n",
    "def custom_ts_multi_data_prep(dataset, target, start, end, window, horizon):\n",
    "    X = []\n",
    "    y = []\n",
    "    start = start + window\n",
    "    if end is None:\n",
    "        end = len(dataset) - horizon\n",
    "\n",
    "    for i in range(start, end):\n",
    "        indices = range(i-window, i)\n",
    "        X.append(dataset[indices])\n",
    "\n",
    "        indicey = range(i+1, i+1+horizon)\n",
    "        y.append(target[indicey])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "df = pd.read_csv(r'..\\Data\\Panama Electricity\\continuous_dataset_preprocessing.csv', parse_dates= True)\n",
    "df[\"datetime\"] = df[\"datetime\"].apply(pd.to_datetime)\n",
    "\n",
    "column_names = df.columns.tolist()\n",
    "train_column_names = df.columns.tolist()\n",
    "column_names.remove('datetime')\n",
    "train_column_names.remove('datetime')\n",
    "train_column_names.remove('nat_demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "731d6ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10348\\4253897158.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['nat_demand'] = train['nat_demand'].clip(lower = 600)\n"
     ]
    }
   ],
   "source": [
    "valid = df[df['datetime'] <= '6/1/2018 23:00']\n",
    "valid = valid[valid['datetime'] > '5/1/2018 23:00']\n",
    "train = df[df['datetime'] <= '5/1/2018 23:00']\n",
    "test = df[df['datetime'] > '6/1/2018 23:00']\n",
    "\n",
    "train['nat_demand'] = train['nat_demand'].clip(lower = 600)\n",
    "valid['nat_demand'] = valid['nat_demand'].clip(lower = 600)\n",
    "\n",
    "# valid = df[df['datetime'] <= '12/3/2019 23:00']\n",
    "# valid = valid[valid['datetime'] > '11/3/2019 23:00']\n",
    "# train = df[df['datetime'] <= '11/3/2019 23:00']\n",
    "# train = train[train['datetime'] > '11/3/2017 23:00']\n",
    "# test = df[df['datetime'] > '12/3/2019 23:00']\n",
    "\n",
    "x_scaler = preprocessing.MinMaxScaler()\n",
    "y_scaler = preprocessing.MinMaxScaler()\n",
    "trainX = x_scaler.fit_transform(train[column_names])\n",
    "trainY = y_scaler.fit_transform(train[['nat_demand']])\n",
    "validX = x_scaler.fit_transform(valid[column_names])\n",
    "validY = y_scaler.fit_transform(valid[['nat_demand']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b169385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 10\n",
    "hist_window = 26\n",
    "horizon = 9\n",
    "\n",
    "x_train_multi, y_train_multi = custom_ts_multi_data_prep(\n",
    "    trainX, trainY, 0, None, hist_window, horizon)\n",
    "x_val_multi, y_val_multi= custom_ts_multi_data_prep(\n",
    "    validX, validY, 0, None, hist_window, horizon)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 150\n",
    "\n",
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e90d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "100/100 [==============================] - 18s 109ms/step - loss: 0.0581 - val_loss: 0.0636\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.0304 - val_loss: 0.0393\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.0217 - val_loss: 0.0270\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 0.0176 - val_loss: 0.0284\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 0.0160 - val_loss: 0.0231\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 0.0137 - val_loss: 0.0199\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 0.0129 - val_loss: 0.0256\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.0130 - val_loss: 0.0241\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0118"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "GRU_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(100, input_shape=x_train_multi.shape[-2:],return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.GRU(units=50,return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=horizon),\n",
    "])\n",
    "GRU_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "folder_save = f'..\\Results\\GRU\\history_{hist_window}_future_{horizon}_version_{version}'\n",
    "isExist = os.path.exists(folder_save)\n",
    "if not isExist:\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(folder_save)\n",
    "   print(\"The new directory is created!\")\n",
    "\n",
    "model_path = f'{folder_save}\\model.h5' \n",
    "\n",
    "with open(f\"{folder_save}\\model_config.json\", \"w\") as outfile:\n",
    "    json.dump(GRU_model.get_config(), outfile)\n",
    "\n",
    "EVALUATION_INTERVAL = 100\n",
    "EPOCHS = 150\n",
    "history = GRU_model.fit(train_data_multi, epochs=EPOCHS,steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                        validation_data=val_data_multi, validation_steps=50,verbose =1,\n",
    "                              callbacks =[tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                                           min_delta=0, patience=15, \n",
    "                                                                           verbose=1, mode='min'),\n",
    "                                          tf.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss',\n",
    "                                                                             save_best_only=True, \n",
    "                                                                             mode='min', verbose=0)])\n",
    "\n",
    "Trained_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper left')\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "plt.savefig(f'{folder_save}\\history_loss.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{folder_save}\\history_loss.svg', bbox_inches='tight')\n",
    "\n",
    "data_test = x_scaler.fit_transform(valid[column_names].tail(hist_window))\n",
    "test_rescaled = data_test.reshape(1, data_test.shape[0], data_test.shape[1])\n",
    "\n",
    "Predicted_results = Trained_model.predict(test_rescaled)\n",
    "Predicted_results_Inv_trans = y_scaler.inverse_transform(Predicted_results)\n",
    "metrics_test = timeseries_evaluation_metrics_func(test['nat_demand'][:horizon],Predicted_results_Inv_trans[0])\n",
    "\n",
    "with open(f\"{folder_save}\\metrics_test.json\", \"w\") as outfile:\n",
    "    json.dump(metrics_test, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e5153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0eb5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bfdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57265029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c8427e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80a238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda05d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a5cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e403360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e136fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1334f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
