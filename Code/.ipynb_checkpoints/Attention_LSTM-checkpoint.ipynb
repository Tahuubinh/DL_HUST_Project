{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f75e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "# tf.random.set_seed(123)\n",
    "# np.random.seed(123)\n",
    "from sklearn import metrics\n",
    "import json \n",
    "import os\n",
    "\n",
    "def timeseries_evaluation_metrics_func(y_true, y_pred):\n",
    "    \n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print('Evaluation metric results:-')\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "    print(f'MSE is : {mse}')\n",
    "    print(f'MAE is : {mae}')\n",
    "    print(f'RMSE is : {rmse}')\n",
    "    print(f'MAPE is : {mape}')\n",
    "    print(f'R2 is : {r2}',end='\\n\\n')\n",
    "    return {'mse' : mse, 'mae' : mae, 'rmse' : rmse, 'mape' : mape, 'r2' : r2}\n",
    "    \n",
    "def custom_ts_multi_data_prep(dataset, target, start, end, window, horizon):\n",
    "    X = []\n",
    "    y = []\n",
    "    start = start + window\n",
    "    if end is None:\n",
    "        end = len(dataset) - horizon\n",
    "\n",
    "    for i in range(start, end):\n",
    "        indices = range(i-window, i)\n",
    "        X.append(dataset[indices])\n",
    "\n",
    "        indicey = range(i+1, i+1+horizon)\n",
    "        y.append(target[indicey])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# df = pd.read_csv(r'..\\Data\\Panama Electricity\\continuous_dataset_preprocessing.csv', parse_dates= True)\n",
    "df = pd.read_csv(\n",
    "    \"/Users/trungnguyen/Desktop/cttn-khmt-k64/deep learning/project/DL_HUST_Project/Data/Panama Electricity/continuous_dataset_preprocessing.csv\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "\n",
    "df[\"datetime\"] = df[\"datetime\"].apply(pd.to_datetime)\n",
    "\n",
    "column_names = df.columns.tolist()\n",
    "train_column_names = df.columns.tolist()\n",
    "column_names.remove('datetime')\n",
    "train_column_names.remove('datetime')\n",
    "train_column_names.remove('nat_demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0746c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = df[df['datetime'] <= '1/1/2019 23:00']\n",
    "valid = valid[valid['datetime'] > '1/1/2018 23:00']\n",
    "train = df[df['datetime'] <= '1/1/2018 23:00']\n",
    "test = df[df['datetime'] > '1/1/2019 23:00']\n",
    "\n",
    "# train['nat_demand'] = train['nat_demand'].clip(lower = 500)\n",
    "# valid['nat_demand'] = valid['nat_demand'].clip(lower = 500)\n",
    "\n",
    "x_scaler = preprocessing.MinMaxScaler()\n",
    "y_scaler = preprocessing.MinMaxScaler()\n",
    "trainX = x_scaler.fit_transform(train[column_names])\n",
    "trainY = y_scaler.fit_transform(train[['nat_demand']])\n",
    "validX = x_scaler.fit_transform(valid[column_names])\n",
    "validY = y_scaler.fit_transform(valid[['nat_demand']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423b79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "hist_window = 48 # 12, 18, 24, 30, 36, 42, 48\n",
    "horizon = 24 # 12, 18, 24, 30, 36\n",
    "\n",
    "x_train_multi, y_train_multi = custom_ts_multi_data_prep(\n",
    "    trainX, trainY, 0, None, hist_window, horizon)\n",
    "x_val_multi, y_val_multi= custom_ts_multi_data_prep(\n",
    "    validX, validY, 0, None, hist_window, horizon)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 150\n",
    "\n",
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8df33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, Dropout, Bidirectional, Multiply\n",
    "from tensorflow.keras.layers import Permute, Flatten\n",
    "#from tensorflow.keras.layers.core import *\n",
    "#from tensorflow.keras.layers.recurrent import LSTM\n",
    "from tensorflow.keras.models import *\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    #a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((1, 2), name='attention_vec')(a)\n",
    "\n",
    "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86065408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new directory is created!\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml_env/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "2023-02-05 22:24:56.299722: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 5s 37ms/step - loss: 0.1114 - val_loss: 0.0617\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0424 - val_loss: 0.0558\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0304 - val_loss: 0.0566\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.0240 - val_loss: 0.0522\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0207 - val_loss: 0.0526\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0183 - val_loss: 0.0527\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0167 - val_loss: 0.0412\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0158 - val_loss: 0.0480\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0148 - val_loss: 0.0444\n",
      "Epoch 10/150\n",
      "  1/100 [..............................] - ETA: 2s - loss: 0.0145"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def attention_model():\n",
    "    inputs = Input(shape=x_train_multi.shape[-2:])\n",
    "\n",
    "#     x = Conv1D(filters = 64, kernel_size = 1, activation = 'relu')(inputs)  #, padding = 'same'\n",
    "#     x = Dropout(0.3)(x)\n",
    "\n",
    "    lstm_out = Bidirectional(LSTM(100, return_sequences=True))(inputs)\n",
    "    lstm_out = Bidirectional(LSTM(50, return_sequences=True))(inputs)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "    #output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    #output = Dense(1, activation='linear')(attention_mul)\n",
    "    output = Dense(20, activation='tanh')(attention_mul)\n",
    "    output = Dropout(0.2)(output)\n",
    "    output = Dense(units=horizon)(output)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model\n",
    "\n",
    "model = attention_model()\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences=True), \n",
    "#                                input_shape=x_train_multi.shape[-2:]),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50)),\n",
    "#     tf.keras.layers.Dense(20, activation='tanh'),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(units=horizon),\n",
    "# ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "project_dir= \"/Users/trungnguyen/Desktop/cttn-khmt-k64/deep learning/project/DL_HUST_Project\"\n",
    "folder_save = project_dir + f\"/Results/Attention_LSTM/history_{hist_window}_future_{horizon}_version_{version}\"\n",
    "\n",
    "isExist = os.path.exists(folder_save)\n",
    "if not isExist:\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(folder_save)\n",
    "   print(\"The new directory is created!\")\n",
    "\n",
    "model_path = f'{folder_save}/model.h5' \n",
    "\n",
    "with open(f\"{folder_save}/model_config.json\", \"w\") as outfile:\n",
    "    json.dump(model.get_config(), outfile)\n",
    "\n",
    "EVALUATION_INTERVAL = 100\n",
    "EPOCHS = 150\n",
    "history = model.fit(train_data_multi, epochs=EPOCHS, steps_per_epoch=EVALUATION_INTERVAL, \n",
    "                         validation_data=val_data_multi, validation_steps=50, verbose=1,\n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                                     min_delta=0, patience=15, \n",
    "                                                                     verbose=1, mode='min'), \n",
    "                                    tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', \n",
    "                                                                       save_best_only=True,\n",
    "                                                                       mode='min', verbose=0)])\n",
    "\n",
    "Trained_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper left')\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "plt.savefig(f'{folder_save}/history_loss.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{folder_save}/history_loss.svg', bbox_inches='tight')\n",
    "history_loss = [list(a) for a in zip(history.history['loss'], history.history['val_loss'])]\n",
    "df_loss = pd.DataFrame(history_loss, columns =['loss', 'val_loss']) \n",
    "df_loss.to_csv(f'{folder_save}/history_loss.csv')\n",
    "\n",
    "data_test = x_scaler.fit_transform(valid[column_names].tail(hist_window))\n",
    "test_rescaled = data_test.reshape(1, data_test.shape[0], data_test.shape[1])\n",
    "\n",
    "Predicted_results = Trained_model.predict(test_rescaled)\n",
    "Predicted_results_Inv_trans = y_scaler.inverse_transform(Predicted_results.reshape(-1,1))\n",
    "metrics_test = timeseries_evaluation_metrics_func(test['nat_demand'][:horizon],Predicted_results_Inv_trans)\n",
    "\n",
    "with open(f\"{folder_save}/metrics_test.json\", \"w\") as outfile:\n",
    "    json.dump(metrics_test, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054110a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
