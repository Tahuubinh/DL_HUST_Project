{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405682da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "# tf.random.set_seed(123)\n",
    "# np.random.seed(123)\n",
    "from sklearn import metrics\n",
    "import json \n",
    "import os\n",
    "\n",
    "def timeseries_evaluation_metrics_func(y_true, y_pred):\n",
    "    \n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print('Evaluation metric results:-')\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "    print(f'MSE is : {mse}')\n",
    "    print(f'MAE is : {mae}')\n",
    "    print(f'RMSE is : {rmse}')\n",
    "    print(f'MAPE is : {mape}')\n",
    "    print(f'R2 is : {r2}',end='\\n\\n')\n",
    "    return {'mse' : mse, 'mae' : mae, 'rmse' : rmse, 'mape' : mape, 'r2' : r2}\n",
    "    \n",
    "def custom_ts_multi_data_prep(dataset, target, start, end, window, horizon):\n",
    "    X = []\n",
    "    y = []\n",
    "    start = start + window\n",
    "    if end is None:\n",
    "        end = len(dataset) - horizon\n",
    "\n",
    "    for i in range(start, end):\n",
    "        indices = range(i-window, i)\n",
    "        X.append(dataset[indices])\n",
    "\n",
    "        indicey = range(i+1, i+1+horizon)\n",
    "        y.append(target[indicey])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "df = pd.read_csv(r'..\\Data\\Panama Electricity\\continuous_dataset_preprocessing.csv', parse_dates= True)\n",
    "df[\"datetime\"] = df[\"datetime\"].apply(pd.to_datetime)\n",
    "\n",
    "column_names = df.columns.tolist()\n",
    "train_column_names = df.columns.tolist()\n",
    "column_names.remove('datetime')\n",
    "train_column_names.remove('datetime')\n",
    "train_column_names.remove('nat_demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ed7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = df[df['datetime'] <= '1/1/2019 23:00']\n",
    "valid = valid[valid['datetime'] > '1/1/2018 23:00']\n",
    "train = df[df['datetime'] <= '1/1/2018 23:00']\n",
    "test = df[df['datetime'] > '1/1/2019 23:00']\n",
    "\n",
    "# train['nat_demand'] = train['nat_demand'].clip(lower = 500)\n",
    "# valid['nat_demand'] = valid['nat_demand'].clip(lower = 500)\n",
    "\n",
    "x_scaler = preprocessing.MinMaxScaler()\n",
    "y_scaler = preprocessing.MinMaxScaler()\n",
    "trainX = x_scaler.fit_transform(train[column_names])\n",
    "trainY = y_scaler.fit_transform(train[['nat_demand']])\n",
    "validX = x_scaler.fit_transform(valid[column_names])\n",
    "validY = y_scaler.fit_transform(valid[['nat_demand']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb77672",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "hist_window = 24 # 12, 18, 24, 30, 36, 42\n",
    "horizon = 24 # 12, 18, 24, 30, 36\n",
    "\n",
    "x_train_multi, y_train_multi = custom_ts_multi_data_prep(\n",
    "    trainX, trainY, 0, None, hist_window, horizon)\n",
    "x_val_multi, y_val_multi= custom_ts_multi_data_prep(\n",
    "    validX, validY, 0, None, hist_window, horizon)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 150\n",
    "\n",
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8541f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, Dropout, Bidirectional, Multiply\n",
    "from tensorflow.keras.layers import Multiply\n",
    "#from tensorflow.keras.layers.core import *\n",
    "#from tensorflow.keras.layers.recurrent import LSTM\n",
    "from tensorflow.keras.models import *\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    #a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((1, 2), name='attention_vec')(a)\n",
    "\n",
    "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "def attention_model():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIMS))\n",
    "\n",
    "    x = Conv1D(filters = 64, kernel_size = 1, activation = 'relu')(inputs)  #, padding = 'same'\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    #lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "    #For GPU you can use CuDNNLSTM\n",
    "    lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "    lstm_out = Dropout(0.3)(lstm_out)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "    #output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    output = Dense(1, activation='linear')(attention_mul)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "ED_lstm_model = attention_model()\n",
    "ED_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "folder_save = f\"../Results/Attention_LSTM/history_{hist_window}_future_{horizon}_version_{version}\"\n",
    "isExist = os.path.exists(folder_save)\n",
    "if not isExist:\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(folder_save)\n",
    "   print(\"The new directory is created!\")\n",
    "\n",
    "model_path = f'{folder_save}/model.h5' \n",
    "\n",
    "with open(f\"{folder_save}/model_config.json\", \"w\") as outfile:\n",
    "    json.dump(ED_lstm_model.get_config(), outfile)\n",
    "\n",
    "EVALUATION_INTERVAL = 100\n",
    "EPOCHS = 150\n",
    "history = ED_lstm_model.fit(train_data_multi, epochs=EPOCHS, steps_per_epoch=EVALUATION_INTERVAL, \n",
    "                         validation_data=val_data_multi, validation_steps=50, verbose=1,\n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                                     min_delta=0, patience=15, \n",
    "                                                                     verbose=1, mode='min'), \n",
    "                                    tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', \n",
    "                                                                       save_best_only=True,\n",
    "                                                                       mode='min', verbose=0)])\n",
    "\n",
    "Trained_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper left')\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "plt.savefig(f'{folder_save}/history_loss.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{folder_save}/history_loss.svg', bbox_inches='tight')\n",
    "history_loss = [list(a) for a in zip(history.history['loss'], history.history['val_loss'])]\n",
    "df_loss = pd.DataFrame(history_loss, columns =['loss', 'val_loss']) \n",
    "df_loss.to_csv(f'{folder_save}/history_loss.csv')\n",
    "\n",
    "data_test = x_scaler.fit_transform(valid[column_names].tail(hist_window))\n",
    "test_rescaled = data_test.reshape(1, data_test.shape[0], data_test.shape[1])\n",
    "\n",
    "Predicted_results = Trained_model.predict(test_rescaled)\n",
    "Predicted_results_Inv_trans = y_scaler.inverse_transform(Predicted_results.reshape(-1,1))\n",
    "metrics_test = timeseries_evaluation_metrics_func(test['nat_demand'][:horizon],Predicted_results_Inv_trans)\n",
    "\n",
    "with open(f\"{folder_save}/metrics_test.json\", \"w\") as outfile:\n",
    "    json.dump(metrics_test, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222a6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6d725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6ccf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba9b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
