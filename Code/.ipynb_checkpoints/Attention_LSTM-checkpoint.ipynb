{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f75e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "# tf.random.set_seed(123)\n",
    "# np.random.seed(123)\n",
    "from sklearn import metrics\n",
    "import json \n",
    "import os\n",
    "\n",
    "def timeseries_evaluation_metrics_func(y_true, y_pred):\n",
    "    \n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print('Evaluation metric results:-')\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "    print(f'MSE is : {mse}')\n",
    "    print(f'MAE is : {mae}')\n",
    "    print(f'RMSE is : {rmse}')\n",
    "    print(f'MAPE is : {mape}')\n",
    "    print(f'R2 is : {r2}',end='\\n\\n')\n",
    "    return {'mse' : mse, 'mae' : mae, 'rmse' : rmse, 'mape' : mape, 'r2' : r2}\n",
    "    \n",
    "def custom_ts_multi_data_prep(dataset, target, start, end, window, horizon):\n",
    "    X = []\n",
    "    y = []\n",
    "    start = start + window\n",
    "    if end is None:\n",
    "        end = len(dataset) - horizon\n",
    "\n",
    "    for i in range(start, end):\n",
    "        indices = range(i-window, i)\n",
    "        X.append(dataset[indices])\n",
    "\n",
    "        indicey = range(i+1, i+1+horizon)\n",
    "        y.append(target[indicey])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# df = pd.read_csv(r'..\\Data\\Panama Electricity\\continuous_dataset_preprocessing.csv', parse_dates= True)\n",
    "df = pd.read_csv(\n",
    "    \"/Users/trungnguyen/Desktop/cttn-khmt-k64/deep learning/project/DL_HUST_Project/Data/Panama Electricity/continuous_dataset_preprocessing.csv\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "\n",
    "df[\"datetime\"] = df[\"datetime\"].apply(pd.to_datetime)\n",
    "\n",
    "column_names = df.columns.tolist()\n",
    "train_column_names = df.columns.tolist()\n",
    "column_names.remove('datetime')\n",
    "train_column_names.remove('datetime')\n",
    "train_column_names.remove('nat_demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0746c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = df[df['datetime'] <= '1/1/2019 23:00']\n",
    "valid = valid[valid['datetime'] > '1/1/2018 23:00']\n",
    "train = df[df['datetime'] <= '1/1/2018 23:00']\n",
    "test = df[df['datetime'] > '1/1/2019 23:00']\n",
    "\n",
    "# train['nat_demand'] = train['nat_demand'].clip(lower = 500)\n",
    "# valid['nat_demand'] = valid['nat_demand'].clip(lower = 500)\n",
    "\n",
    "x_scaler = preprocessing.MinMaxScaler()\n",
    "y_scaler = preprocessing.MinMaxScaler()\n",
    "trainX = x_scaler.fit_transform(train[column_names])\n",
    "trainY = y_scaler.fit_transform(train[['nat_demand']])\n",
    "validX = x_scaler.fit_transform(valid[column_names])\n",
    "validY = y_scaler.fit_transform(valid[['nat_demand']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423b79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "hist_window = 24 # 12, 18, 24, 30, 36, 42\n",
    "horizon = 24 # 12, 18, 24, 30, 36\n",
    "\n",
    "x_train_multi, y_train_multi = custom_ts_multi_data_prep(\n",
    "    trainX, trainY, 0, None, hist_window, horizon)\n",
    "x_val_multi, y_val_multi= custom_ts_multi_data_prep(\n",
    "    validX, validY, 0, None, hist_window, horizon)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 150\n",
    "\n",
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8df33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, Dropout, Bidirectional, Multiply\n",
    "from tensorflow.keras.layers import Permute, Flatten\n",
    "#from tensorflow.keras.layers.core import *\n",
    "#from tensorflow.keras.layers.recurrent import LSTM\n",
    "from tensorflow.keras.models import *\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    #a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((1, 2), name='attention_vec')(a)\n",
    "\n",
    "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86065408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new directory is created!\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml_env/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "2023-01-31 14:18:25.410303: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 0.1073"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Failed to format this callback filepath: \"/Users/trungnguyen/Desktop/cttn-khmt-k64/deep learning/project/DL_HUST_Project/Results/Attention_LSTM/history_{hist_window}_future_{horizon}_version_{version}/model.h5\". Reason: \\'hist_window\\''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m EVALUATION_INTERVAL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     49\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n\u001b[0;32m---> 50\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_multi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVALUATION_INTERVAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data_multi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m Trained_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model_path)\n\u001b[1;32m     61\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml_env/lib/python3.9/site-packages/keras/callbacks.py:1471\u001b[0m, in \u001b[0;36mModelCheckpoint._get_file_path\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1468\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1469\u001b[0m         epoch\u001b[38;5;241m=\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, batch\u001b[38;5;241m=\u001b[39mbatch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlogs)\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1471\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1472\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to format this callback filepath: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1473\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReason: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_filepath \u001b[38;5;241m=\u001b[39m distributed_file_utils\u001b[38;5;241m.\u001b[39mwrite_filepath(\n\u001b[1;32m   1475\u001b[0m     file_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdistribute_strategy)\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_filepath\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Failed to format this callback filepath: \"/Users/trungnguyen/Desktop/cttn-khmt-k64/deep learning/project/DL_HUST_Project/Results/Attention_LSTM/history_{hist_window}_future_{horizon}_version_{version}/model.h5\". Reason: \\'hist_window\\''"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def attention_model():\n",
    "    inputs = Input(shape=x_train_multi.shape[-2:])\n",
    "\n",
    "#     x = Conv1D(filters = 64, kernel_size = 1, activation = 'relu')(inputs)  #, padding = 'same'\n",
    "#     x = Dropout(0.3)(x)\n",
    "\n",
    "    lstm_out = Bidirectional(LSTM(100, return_sequences=True))(inputs)\n",
    "    lstm_out = Bidirectional(LSTM(50, return_sequences=True))(inputs)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "    #output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    #output = Dense(1, activation='linear')(attention_mul)\n",
    "    output = Dense(20, activation='tanh')(attention_mul)\n",
    "    output = Dropout(0.2)(output)\n",
    "    output = Dense(units=horizon)(output)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model\n",
    "\n",
    "model = attention_model()\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences=True), \n",
    "#                                input_shape=x_train_multi.shape[-2:]),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50)),\n",
    "#     tf.keras.layers.Dense(20, activation='tanh'),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(units=horizon),\n",
    "# ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "project_dir= \"/Users/trungnguyen/Desktop/cttn-khmt-k64/deep learning/project/DL_HUST_Project\"\n",
    "folder_save = project_dir + f\"/Results/Attention_LSTM/history_{hist_window}_future_{horizon}_version_{version}\"\n",
    "\n",
    "isExist = os.path.exists(folder_save)\n",
    "if not isExist:\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(folder_save)\n",
    "   print(\"The new directory is created!\")\n",
    "\n",
    "model_path = f'{folder_save}/model.h5' \n",
    "\n",
    "with open(f\"{folder_save}/model_config.json\", \"w\") as outfile:\n",
    "    json.dump(model.get_config(), outfile)\n",
    "\n",
    "EVALUATION_INTERVAL = 100\n",
    "EPOCHS = 150\n",
    "history = model.fit(train_data_multi, epochs=EPOCHS, steps_per_epoch=EVALUATION_INTERVAL, \n",
    "                         validation_data=val_data_multi, validation_steps=50, verbose=1,\n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                                     min_delta=0, patience=15, \n",
    "                                                                     verbose=1, mode='min'), \n",
    "                                    tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', \n",
    "                                                                       save_best_only=True,\n",
    "                                                                       mode='min', verbose=0)])\n",
    "\n",
    "Trained_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper left')\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "plt.savefig(f'{folder_save}/history_loss.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'{folder_save}/history_loss.svg', bbox_inches='tight')\n",
    "history_loss = [list(a) for a in zip(history.history['loss'], history.history['val_loss'])]\n",
    "df_loss = pd.DataFrame(history_loss, columns =['loss', 'val_loss']) \n",
    "df_loss.to_csv(f'{folder_save}/history_loss.csv')\n",
    "\n",
    "data_test = x_scaler.fit_transform(valid[column_names].tail(hist_window))\n",
    "test_rescaled = data_test.reshape(1, data_test.shape[0], data_test.shape[1])\n",
    "\n",
    "Predicted_results = Trained_model.predict(test_rescaled)\n",
    "Predicted_results_Inv_trans = y_scaler.inverse_transform(Predicted_results.reshape(-1,1))\n",
    "metrics_test = timeseries_evaluation_metrics_func(test['nat_demand'][:horizon],Predicted_results_Inv_trans)\n",
    "\n",
    "with open(f\"{folder_save}/metrics_test.json\", \"w\") as outfile:\n",
    "    json.dump(metrics_test, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054110a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
